{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cbcd0e51",
      "metadata": {
        "id": "cbcd0e51"
      },
      "outputs": [],
      "source": [
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "AxFtbITalOXd"
      },
      "id": "AxFtbITalOXd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import xml.etree.ElementTree as ET\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "import random\n",
        "import tensorflow as tf\n",
        "import keras_cv\n"
      ],
      "metadata": {
        "id": "KI63ZdUdtzZZ"
      },
      "id": "KI63ZdUdtzZZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pathlib\n",
        "\n",
        "base_dir = pathlib.Path(\"/content/drive/MyDrive/Coincent Internship Project/VOC2012\")\n",
        "img_dir = base_dir / \"JPEGImages\"\n",
        "ann_dir = base_dir / \"Annotations\"\n",
        "sets_dir = base_dir / \"ImageSets\" / \"Main\"\n",
        "\n",
        "import os\n",
        "print(\"Images:\", len(os.listdir(img_dir)))\n",
        "print(\"Annotations:\", len(os.listdir(ann_dir)))\n",
        "\n",
        "assert img_dir.exists(), \"Image directory missing\"\n",
        "assert ann_dir.exists(), \"Annotations directory missing\""
      ],
      "metadata": {
        "id": "ebIrTORQuQ3u"
      },
      "id": "ebIrTORQuQ3u",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ebcc9d7",
      "metadata": {
        "id": "7ebcc9d7"
      },
      "outputs": [],
      "source": [
        "split_file = sets_dir / \"trainval.txt\"\n",
        "if not split_file.exists():\n",
        "    # Fallback: build a split\n",
        "    all_ids = [p.stem for p in img_dir.glob(\"*.jpg\")]\n",
        "    random.shuffle(all_ids)\n",
        "    split = int(0.8 * len(all_ids))\n",
        "    train_ids, val_ids = all_ids[:split], all_ids[split:]\n",
        "else:\n",
        "    with open(split_file) as f:\n",
        "        all_ids = [line.strip() for line in f if line.strip()]\n",
        "    random.shuffle(all_ids)\n",
        "    split = int(0.8 * len(all_ids))\n",
        "    train_ids, val_ids = all_ids[:split], all_ids[split:]\n",
        "\n",
        "print(\"Train images:\", len(train_ids), \"Val images:\", len(val_ids))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6aab37a4",
      "metadata": {
        "id": "6aab37a4"
      },
      "outputs": [],
      "source": [
        "# VOC classes\n",
        "VOC_CLASSES = [\n",
        "    \"aeroplane\",\"bicycle\",\"bird\",\"boat\",\"bottle\",\n",
        "    \"bus\",\"car\",\"cat\",\"chair\",\"cow\",\n",
        "    \"diningtable\",\"dog\",\"horse\",\"motorbike\",\"person\",\n",
        "    \"pottedplant\",\"sheep\",\"sofa\",\"train\",\"tvmonitor\"\n",
        "]\n",
        "cls_to_id = {c:i for i,c in enumerate(VOC_CLASSES)}\n",
        "\n",
        "def parse_voc_xml(xml_path):\n",
        "    tree = ET.parse(xml_path)\n",
        "    root = tree.getroot()\n",
        "    size = root.find(\"size\")\n",
        "    w = int(size.find(\"width\").text)\n",
        "    h = int(size.find(\"height\").text)\n",
        "    objs = []\n",
        "    for obj in root.findall(\"object\"):\n",
        "        name = obj.find(\"name\").text\n",
        "        difficult = int(obj.find(\"difficult\").text) if obj.find(\"difficult\") is not None else 0\n",
        "        bnd = obj.find(\"bndbox\")\n",
        "        xmin = int(float(bnd.find(\"xmin\").text))\n",
        "        ymin = int(float(bnd.find(\"ymin\").text))\n",
        "        xmax = int(float(bnd.find(\"xmax\").text))\n",
        "        ymax = int(float(bnd.find(\"ymax\").text))\n",
        "        objs.append({\n",
        "            \"label\": name,\n",
        "            \"difficult\": difficult,\n",
        "            \"bbox\": [xmin, ymin, xmax, ymax],\n",
        "        })\n",
        "    return w, h, objs\n",
        "\n",
        "def build_records(id_list):\n",
        "    recs = []\n",
        "    for img_id in tqdm(id_list, desc=\"Parsing\"):\n",
        "        img_path = img_dir / f\"{img_id}.jpg\"\n",
        "        xml_path = ann_dir / f\"{img_id}.xml\"\n",
        "        if not xml_path.exists() or not img_path.exists():\n",
        "            continue\n",
        "        w,h,objs = parse_voc_xml(xml_path)\n",
        "        for o in objs:\n",
        "            if o[\"label\"] not in cls_to_id:\n",
        "                continue\n",
        "            recs.append({\n",
        "                \"image_id\": img_id,\n",
        "                \"image_path\": str(img_path),\n",
        "                \"width\": w, \"height\": h,\n",
        "                \"xmin\": o[\"bbox\"][0], \"ymin\": o[\"bbox\"][1],\n",
        "                \"xmax\": o[\"bbox\"][2], \"ymax\": o[\"bbox\"][3],\n",
        "                \"label\": o[\"label\"],\n",
        "                \"label_id\": cls_to_id[o[\"label\"]],\n",
        "                \"difficult\": o[\"difficult\"]\n",
        "            })\n",
        "    return pd.DataFrame(recs)\n",
        "\n",
        "df_train = build_records(train_ids)\n",
        "df_val   = build_records(val_ids)\n",
        "print(df_train.head())\n",
        "print(\"Train boxes:\", len(df_train), \"Val boxes:\", len(df_val))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "IMG_SIZE = 500\n",
        "\n",
        "def preprocess(row):\n",
        "    img = tf.io.read_file(row[\"image_path\"])\n",
        "    img = tf.image.decode_jpeg(img, channels=3)\n",
        "    img = tf.image.resize(img, (IMG_SIZE, IMG_SIZE)) / 255.0\n",
        "\n",
        "    x_min, y_min, x_max, y_max = [row[k] for k in [\"xmin\",\"ymin\",\"xmax\",\"ymax\"]]\n",
        "    boxes = tf.convert_to_tensor([[x_min/row[\"width\"], y_min/row[\"height\"],\n",
        "                                   x_max/row[\"width\"], y_max/row[\"height\"]]], dtype=tf.float32)\n",
        "    classes = tf.convert_to_tensor([row[\"label_id\"]], dtype=tf.int32)\n",
        "\n",
        "    return {\"images\": img,\n",
        "            \"bounding_boxes\": {\"boxes\": boxes, \"classes\": classes}}\n",
        "\n",
        "def make_tf_dataset(df, shuffle=True):\n",
        "    ds = tf.data.Dataset.from_tensor_slices(dict(df))\n",
        "    if shuffle: ds = ds.shuffle(512)\n",
        "    ds = ds.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    ds = ds.padded_batch(8)\n",
        "    return ds.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "train_ds = make_tf_dataset(df_train)\n",
        "val_ds   = make_tf_dataset(df_val, shuffle=False)\n"
      ],
      "metadata": {
        "id": "BNACike2BQEY"
      },
      "id": "BNACike2BQEY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_tf_dataset(df, shuffle=True, batch_size=8):\n",
        "    def gen():\n",
        "        for _, row in df.iterrows():\n",
        "            img = tf.io.read_file(row[\"image_path\"])\n",
        "            img = tf.image.decode_jpeg(img, channels=3)\n",
        "            img = tf.image.convert_image_dtype(img, tf.float32)  # normalize [0,1]\n",
        "\n",
        "            # normalize bbox to [0,1] relative to width & height\n",
        "            xmin = row[\"xmin\"] / row[\"width\"]\n",
        "            ymin = row[\"ymin\"] / row[\"height\"]\n",
        "            xmax = row[\"xmax\"] / row[\"width\"]\n",
        "            ymax = row[\"ymax\"] / row[\"height\"]\n",
        "\n",
        "            yield {\n",
        "                \"images\": img,\n",
        "                \"bounding_boxes\": {\n",
        "                    \"boxes\": tf.constant([[xmin, ymin, xmax, ymax]], dtype=tf.float32),\n",
        "                    \"classes\": tf.constant([row[\"label_id\"]], dtype=tf.int32),\n",
        "                },\n",
        "            }\n",
        "\n",
        "    ds = tf.data.Dataset.from_generator(\n",
        "        gen,\n",
        "        output_signature={\n",
        "            \"images\": tf.TensorSpec(shape=(None, None, 3), dtype=tf.float32),\n",
        "            \"bounding_boxes\": {\n",
        "                \"boxes\": tf.TensorSpec(shape=(None, 4), dtype=tf.float32),\n",
        "                \"classes\": tf.TensorSpec(shape=(None,), dtype=tf.int32),\n",
        "            },\n",
        "        },\n",
        "    )\n",
        "    if shuffle:\n",
        "        ds = ds.shuffle(512)\n",
        "    return ds.padded_batch(batch_size).prefetch(tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "44bokFuawuhF"
      },
      "id": "44bokFuawuhF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = make_tf_dataset(df_train)\n",
        "val_ds   = make_tf_dataset(df_val, shuffle=False)\n"
      ],
      "metadata": {
        "id": "LydLk_0AyLG4"
      },
      "id": "LydLk_0AyLG4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch = next(iter(train_ds))\n",
        "\n",
        "images = batch[\"images\"]\n",
        "boxes = batch[\"bounding_boxes\"][\"boxes\"]\n",
        "classes = batch[\"bounding_boxes\"][\"classes\"]\n",
        "\n",
        "print(\"Batch image shape:\", images.shape)\n",
        "print(\"Batch boxes shape:\", boxes.shape)\n",
        "print(\"Batch classes shape:\", classes.shape)\n"
      ],
      "metadata": {
        "id": "6ZK37zgXxSOl"
      },
      "id": "6ZK37zgXxSOl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23b0ef0a"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "\n",
        "IMG_SIZE = 512\n",
        "BATCH_SIZE = 8\n",
        "\n",
        "def grouped_records(df):\n",
        "    groups = {}\n",
        "    for _, row in df.iterrows():\n",
        "        img_id = row[\"image_id\"]\n",
        "        groups.setdefault(img_id, []).append(row)\n",
        "    records = []\n",
        "    for img_id, rows in groups.items():\n",
        "        first = rows[0]\n",
        "        img_path = first[\"image_path\"]\n",
        "        orig_w = float(first[\"width\"])\n",
        "        orig_h = float(first[\"height\"])\n",
        "        boxes = []\n",
        "        classes = []\n",
        "        for r in rows:\n",
        "            boxes.append([r[\"xmin\"], r[\"ymin\"], r[\"xmax\"], r[\"ymax\"]])\n",
        "            classes.append(int(r[\"label_id\"]))\n",
        "        boxes = np.array(boxes, dtype=np.float32)   # (N,4)\n",
        "        classes = np.array(classes, dtype=np.int32) # (N,)\n",
        "        records.append({\n",
        "            \"image_path\": img_path,\n",
        "            \"orig_w\": orig_w,\n",
        "            \"orig_h\": orig_h,\n",
        "            \"boxes\": boxes,\n",
        "            \"classes\": classes\n",
        "        })\n",
        "    return records\n",
        "\n",
        "train_records = grouped_records(df_train)\n",
        "val_records   = grouped_records(df_val)\n",
        "\n",
        "def generator(records):\n",
        "    for rec in records:\n",
        "\n",
        "        img = tf.io.read_file(rec[\"image_path\"])\n",
        "        img = tf.image.decode_jpeg(img, channels=3)\n",
        "        img = tf.image.convert_image_dtype(img, tf.float32)\n",
        "        boxes = rec[\"boxes\"].astype(np.float32)\n",
        "        boxes[:, 0] /= rec[\"orig_w\"]   # xmin / w\n",
        "        boxes[:, 2] /= rec[\"orig_w\"]   # xmax / w\n",
        "        boxes[:, 1] /= rec[\"orig_h\"]   # ymin / h\n",
        "        boxes[:, 3] /= rec[\"orig_h\"]   # ymax / h\n",
        "\n",
        "        # resize image to fixed size\n",
        "        img = tf.image.resize(img, (IMG_SIZE, IMG_SIZE))\n",
        "\n",
        "        yield {\n",
        "            \"images\": img,\n",
        "            \"bounding_boxes\": {\n",
        "                \"boxes\": tf.convert_to_tensor(boxes, dtype=tf.float32),   # (N,4)\n",
        "                \"classes\": tf.convert_to_tensor(rec[\"classes\"], dtype=tf.int32), # (N,)\n",
        "            }\n",
        "        }\n",
        "\n",
        "output_signature = {\n",
        "    \"images\": tf.TensorSpec(shape=(IMG_SIZE, IMG_SIZE, 3), dtype=tf.float32),\n",
        "    \"bounding_boxes\": {\n",
        "        \"boxes\": tf.TensorSpec(shape=(None, 4), dtype=tf.float32),\n",
        "        \"classes\": tf.TensorSpec(shape=(None,), dtype=tf.int32),\n",
        "    }\n",
        "}\n",
        "\n",
        "train_ds = tf.data.Dataset.from_generator(lambda: generator(train_records), output_signature=output_signature)\n",
        "val_ds   = tf.data.Dataset.from_generator(lambda: generator(val_records), output_signature=output_signature)\n",
        "\n",
        "# shuffle, padded batch (images are already fixed-size, only boxes/classes need padding)\n",
        "padding_values = {\n",
        "    \"images\": tf.constant(0.0, dtype=tf.float32),\n",
        "    \"bounding_boxes\": {\n",
        "        \"boxes\": tf.constant(0.0, dtype=tf.float32),\n",
        "        \"classes\": tf.constant(-1, dtype=tf.int32),\n",
        "    }\n",
        "}\n",
        "\n",
        "train_ds = train_ds.shuffle(512).padded_batch(BATCH_SIZE, padding_values=padding_values).prefetch(tf.data.AUTOTUNE)\n",
        "val_ds   = val_ds.padded_batch(BATCH_SIZE, padding_values=padding_values).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "batch = next(iter(train_ds))\n",
        "print(\"images:\", batch[\"images\"].shape)                          # (B, IMG_SIZE, IMG_SIZE, 3)\n",
        "print(\"boxes:\", batch[\"bounding_boxes\"][\"boxes\"].shape)          # (B, max_objects, 4)\n",
        "print(\"classes:\", batch[\"bounding_boxes\"][\"classes\"].shape)      # (B, max_objects)\n",
        "\n",
        "\n",
        "def show_batch(batch, VOC_CLASSES, n=4):\n",
        "    imgs = batch[\"images\"].numpy()\n",
        "    boxes = batch[\"bounding_boxes\"][\"boxes\"].numpy()\n",
        "    classes = batch[\"bounding_boxes\"][\"classes\"].numpy()\n",
        "    plt.figure(figsize=(12,12))\n",
        "    for i in range(min(n, imgs.shape[0])):\n",
        "        img = imgs[i]\n",
        "        plt.subplot(2,2,i+1)\n",
        "        plt.imshow(img)\n",
        "        h, w = IMG_SIZE, IMG_SIZE\n",
        "        for j in range(boxes.shape[1]):\n",
        "            cls = int(classes[i,j])\n",
        "            if cls < 0:\n",
        "                continue\n",
        "            xmin, ymin, xmax, ymax = boxes[i,j]\n",
        "            # scaled coords\n",
        "            x1, y1, x2, y2 = xmin * w, ymin * h, xmax * w, ymax * h\n",
        "            rect = patches.Rectangle((x1, y1), x2-x1, y2-y1, linewidth=2, edgecolor='r', facecolor='none')\n",
        "            plt.gca().add_patch(rect)\n",
        "            plt.text(x1, y1-6, VOC_CLASSES[cls], color='yellow', fontsize=9, bbox=dict(facecolor='black', alpha=0.6))\n",
        "        plt.axis(\"off\")\n",
        "    plt.show()\n",
        "\n",
        "import matplotlib.patches as patches\n",
        "show_batch(batch, VOC_CLASSES)\n"
      ],
      "id": "23b0ef0a",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}